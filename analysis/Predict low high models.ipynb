{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict low high models\n",
    "Model selection for predict lo hi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install conda and binance packages to this notebook uncomment the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%conda install -c plotly plotly=5.9.0\n",
    "#%conda install pip\n",
    "#%conda install twisted\n",
    "%pip install plotly==5.9.0\n",
    "%pip install twisted\n",
    "#%pip install binance-connector==1.13.0\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install tensorflow==2.11.0\n",
    "%pip install keras==2.11.0\n",
    "%pip install scikeras\n",
    "%pip install keras-tuner\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Read data from bot's history, resample to equal intervals, create X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from bot's history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./../data/yandex-cloud/LSTMStrategy/Xy/2023-05-27_BTCUSDT_data.csv', './../data/yandex-cloud/LSTMStrategy/Xy/2023-05-28_BTCUSDT_data.csv']\n",
      "['./../data/yandex-cloud/LSTMStrategy/Xy/2023-05-27_BTCUSDT_X.csv', './../data/yandex-cloud/LSTMStrategy/Xy/2023-05-28_BTCUSDT_X.csv']\n",
      "['./../data/yandex-cloud/LSTMStrategy/Xy/2023-05-27_BTCUSDT_y.csv', './../data/yandex-cloud/LSTMStrategy/Xy/2023-05-28_BTCUSDT_y.csv']\n",
      "Raw data len: 295809\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import timedelta,date\n",
    "\n",
    "def read_last_data(strategy, days=1, n=None):\n",
    "    \"\"\" Read last last day from bot history \"\"\"\n",
    "    \n",
    "    def read_last(symbol: str, name: str, days=1):\n",
    "        data_dir=f\"./../data/yandex-cloud/{strategy}/Xy\"\n",
    "        file_paths = sorted([f\"{data_dir}/{f}\" for f in os.listdir(data_dir) if f.endswith(f\"{symbol}_{name}.csv\")])[-days:]\n",
    "        print(file_paths)\n",
    "        return pd.concat([pd.read_csv(f, parse_dates=True, index_col=\"datetime\") for f in file_paths])\n",
    "\n",
    "    data=read_last(\"BTCUSDT\", \"data\", days)\n",
    "    X=read_last(\"BTCUSDT\", \"X\", days)\n",
    "    y=read_last(\"BTCUSDT\", \"y\", days)\n",
    "    if n: \n",
    "        data=data.tail(n)\n",
    "        X=X.tail(n)\n",
    "        y=y.tail(n)\n",
    "    return data,X,y\n",
    "\n",
    "#strategy=\"SimpleKerasStrategy\"\n",
    "strategy=\"LSTMStrategy\"\n",
    "bidask,X_bot,y_bot = read_last_data(strategy, days=2, n=1000000)\n",
    "print(f\"Raw data len: {len(X_bot)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample data to use equal time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X len: 295809, y len: 296051\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def get_targets(bidask, predict_window=\"10s\"):\n",
    "    \"\"\" \n",
    "    Calculate targets - bid/ask bounds in future prediction window\n",
    "    \"\"\"\n",
    "    fut_min=bidask[[\"bid\", \"ask\"]][::-1].rolling(predict_window).min()[::-1].rename(columns={\"bid\":\"bid_min_fut\", \"ask\":\"ask_min_fut\"})\n",
    "    fut_max=bidask[[\"bid\", \"ask\"]][::-1].rolling(predict_window).max()[::-1].rename(columns={\"bid\":\"bid_max_fut\", \"ask\":\"ask_max_fut\"})\n",
    "    return pd.concat([fut_min, fut_max], axis = 1)\n",
    "    \n",
    "def resampled(X: pd.DataFrame, y: pd.DataFrame, time_interval) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\" Resample to make equal intervals time series \"\"\"\n",
    "    if int(re.sub(r'\\D', \"\", time_interval)) == 0:\n",
    "        return X, y\n",
    "    Xy=pd.concat([X,y], axis=1)\n",
    "    diffmap = [(c, \"sum\") for c in Xy.columns if c.endswith(\"diff\")]\n",
    "    timemap = [(c, \"last\") for c in Xy.columns if c.startswith(\"time\") and not c.endswith(\"diff\")]\n",
    "    l2map = [(c, \"last\") for c in Xy.columns if c.startswith(\"l2_\")]\n",
    "    candlemap = [(c,\"last\") for c in Xy.columns \\\n",
    "                 if c.endswith(\"_open\") or c.endswith(\"_high\") or c.endswith(\"_low\") or c.endswith(\"_close\") or c.endswith(\"_vol\")]\n",
    "    \n",
    "    futmap = [(c, \"last\") for c in Xy.columns if c.endswith(\"_fut\")]\n",
    "    colmap = dict(diffmap + timemap + l2map + futmap + [(\"spread\", \"last\")] + candlemap)\n",
    "    resampled = Xy.resample(time_interval).agg(colmap).dropna()\n",
    "    return resampled[X.columns], resampled[y.columns]\n",
    "\n",
    "predict_window=\"60s\"\n",
    "resample_interval=\"10s\"\n",
    "y_raw = get_targets(bidask, predict_window)\n",
    "X,y=X_bot,y_raw\n",
    "#X,y = resampled(X_bot, y_raw, resample_interval)\n",
    "print(f\"X len: {len(X)}, y len: {len(y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 207066, test size: 88743\n",
      "X shape: (207066, 80), y shape: (207066, 4)\n",
      "X shape: (88743, 80), y shape: (88985, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.layers import *\n",
    "from keras.layers import LSTM\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "\n",
    "def train_test_split(X,y):\n",
    "    test_size=0.3\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    X_train, y_train, X_test, y_test = X.iloc[:test_index], y.iloc[:test_index], X.iloc[test_index:], y.iloc[test_index:]\n",
    "    time_cols=[col for col in X.columns if col.startswith(\"time\")]\n",
    "    float_cols = list(set(X.columns)-set(time_cols))\n",
    "\n",
    "    # Train/test split    \n",
    "#     x_scaler = \n",
    "\n",
    "    x_scaler = ColumnTransformer([(\"xrs\",RobustScaler(), float_cols)], remainder=\"passthrough\")\n",
    "\n",
    "    x_pipe = Pipeline(\n",
    "        [(\"xscaler\", ColumnTransformer([(\"xrs\",RobustScaler(), float_cols)], remainder=\"passthrough\")),\n",
    "         (\"xmms\",MinMaxScaler())])\n",
    "\n",
    "    \n",
    "    x_pipe.fit(X_train)\n",
    "\n",
    "    y_pipe = Pipeline(\n",
    "        [(\"yrs\", RobustScaler()),\n",
    "         (\"ymms\",MinMaxScaler())])\n",
    "    y_pipe.fit(y_train)\n",
    "\n",
    "    #x_scaler, y_scaler = MinMaxScaler().fit(X_train), MinMaxScaler().fit(y_train)\n",
    "    X_train, y_train = x_pipe.transform(X_train), y_pipe.transform(y_train)\n",
    "    X_test, y_test = x_pipe.transform(X_test), y_pipe.transform(y_test)\n",
    "    print(f\"Train size: {X_train.shape[0]}, test size: {X_test.shape[0]}\")\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "print(f\"X shape: {X_train.shape}, y shape: {y_train.shape}\")\n",
    "print(f\"X shape: {X_test.shape}, y shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.layers import *\n",
    "from keras.layers import LSTM\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data and targets have to be of same length. Data length is 88743 while target length is 88985",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 105\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tuner\n\u001b[1;32m    104\u001b[0m train_gen \u001b[38;5;241m=\u001b[39m TimeseriesGenerator(X_train, y_train, length\u001b[38;5;241m=\u001b[39mwindow_size)\n\u001b[0;32m--> 105\u001b[0m test_gen \u001b[38;5;241m=\u001b[39m \u001b[43mTimeseriesGenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwindow_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Uncomment to do optimize\u001b[39;00m\n\u001b[1;32m    107\u001b[0m tuner \u001b[38;5;241m=\u001b[39m tuner_search(\u001b[38;5;241m1\u001b[39m, train_gen, test_gen)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/preprocessing/sequence.py:138\u001b[0m, in \u001b[0;36mTimeseriesGenerator.__init__\u001b[0;34m(self, data, targets, length, sampling_rate, stride, start_index, end_index, shuffle, reverse, batch_size)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    125\u001b[0m     data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    134\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m    135\u001b[0m ):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(targets):\n\u001b[0;32m--> 138\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData and targets have to be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    140\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m of same length. Data length is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m             \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m while target length is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(targets)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    142\u001b[0m         )\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m targets\n",
      "\u001b[0;31mValueError\u001b[0m: Data and targets have to be of same length. Data length is 88743 while target length is 88985"
     ]
    }
   ],
   "source": [
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.layers import *\n",
    "from keras.layers import LSTM\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_tuner_model(hp):\n",
    "    \"\"\" Create model for tuner. Expects X_train, y_train, window_size\"\"\"\n",
    "    model = Sequential()\n",
    "    hp_lstm1_units=hp.Int(\"lstm1_units\", min_value=4, max_value=1000)\n",
    "    hp_lstm1_dropout=hp.Float(\"lstm1_dropout\", min_value=0, max_value=0.3)\n",
    "\n",
    "#     column_num=X_train.shape[1]\n",
    "#     window_sizes=[1,5,10,50] \n",
    "#     hp_window_size=hp.Choice(\"window_size\", window_sizes)\n",
    "    model.add(LSTM(hp_lstm1_units,\n",
    "                   return_sequences=True, input_shape=(window_size, X_train.shape[1])))\n",
    "#     model.add(LSTM(hp_input_units,\n",
    "#                    return_sequences=True, input_shape=(hp_window_size, column_num)))\n",
    "\n",
    "    model.add(Dropout(hp_lstm1_dropout))\n",
    "    \n",
    "    hp_lstm2_units=hp.Int(\"lstm2_units\", min_value=4, max_value=1000)\n",
    "    hp_lstm2_dropout=hp.Float(\"lstm2_dropout\", min_value=0, max_value=0.3)\n",
    "    model.add(LSTM(hp_lstm2_units))         \n",
    "    model.add(Dropout(hp_lstm2_dropout))\n",
    "\n",
    "    hp_dense1_units=hp.Int(\"dense1_units\", min_value=4, max_value=1000)\n",
    "    hp_dense1_dropout=hp.Float(\"dense1_dropout\", min_value=0, max_value=0.3)\n",
    "    model.add(Dense(hp_dense1_units, activation='relu'))\n",
    "    model.add(Dropout(hp_dense1_dropout))\n",
    "\n",
    "    hp_dense2_units=hp.Int(\"dense2_units\", min_value=4, max_value=100)\n",
    "    hp_dense2_dropout=hp.Float(\"dense2_dropout\", min_value=0, max_value=0.3)\n",
    "    model.add(Dense(hp_dense2_units, activation='relu'))\n",
    "    model.add(Dropout(hp_dense2_dropout))\n",
    "    \n",
    "    model.add(Dense(y_train.shape[1], activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "    return model\n",
    "    \n",
    "window_size = 10\n",
    "\n",
    "def create_tuner(max_trials):\n",
    "    # Tune the model\n",
    "    # tuner=keras_tuner.Hyperband(\n",
    "    #     hypermodel=create_tuner_model,\n",
    "    #     objective=\"val_mse\",\n",
    "    #     max_epochs=5,\n",
    "    #     factor=3,\n",
    "    #     hyperband_iterations=1,\n",
    "    #     seed=None,\n",
    "    #     hyperparameters=None,\n",
    "    #     tune_new_entries=True,\n",
    "    #     allow_new_entries=True,\n",
    "    #     max_retries_per_trial=0,\n",
    "    #     max_consecutive_failed_trials=3\n",
    "    # )\n",
    "    tuner=keras_tuner.BayesianOptimization(\n",
    "        hypermodel=create_tuner_model,\n",
    "        objective=\"val_mse\",\n",
    "        max_trials=max_trials,\n",
    "        num_initial_points=None,\n",
    "        alpha=0.0001,\n",
    "        beta=2.6,\n",
    "        seed=None,\n",
    "        hyperparameters=None,\n",
    "        tune_new_entries=True,\n",
    "        allow_new_entries=True,\n",
    "        max_retries_per_trial=0,\n",
    "        max_consecutive_failed_trials=3,\n",
    "        overwrite=True,\n",
    "        directory=f\"./tmp/{strategy}/tune\",\n",
    "        project_name=\"pytrade2\",    \n",
    "    )\n",
    "    # tuner = keras_tuner.RandomSearch(\n",
    "    #     hypermodel=create_tuner_model,\n",
    "    #     objective=\"val_mse\",\n",
    "    #     max_trials=3,\n",
    "    #     executions_per_trial=2,\n",
    "    #     overwrite=True,\n",
    "    #     directory=f\"./tmp/{strategy}/tune\",\n",
    "    #     project_name=\"pytrade2\",\n",
    "    #     )\n",
    "    tuner.search_space_summary()\n",
    "    return tuner\n",
    "\n",
    "def tuner_search(max_trials,train_gen, test_gen):\n",
    "    tuner = create_tuner(max_trials)\n",
    "    tuner.search(train_gen,epochs=5, validation_data=test_gen)\n",
    "    return tuner\n",
    "\n",
    "train_gen = TimeseriesGenerator(X_train, y_train, length=window_size)\n",
    "test_gen = TimeseriesGenerator(X_test, y_test, length=window_size)\n",
    "# Uncomment to do optimize\n",
    "tuner = tuner_search(1, train_gen, test_gen)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"1\")\n",
    "\n",
    "tuner.results_summary()\n",
    "models=tuner.get_best_models(5)\n",
    "for model in models:\n",
    "    model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# def create_model_prod(X_train, y_train, window_size):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(128,  return_sequences=True, input_shape=(window_size, X_train.shape[1])))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(LSTM(32))         \n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(20, activation='relu'))\n",
    "#     model.add(Dense(y_train.shape[1], activation='linear'))\n",
    "#     #model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "#     model.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "#     return model  \n",
    "plot_figsize=(10,5)\n",
    "#def create_model(X_train, y_train, window_size, lstm1_units, lstm2_units, dense1_units, dense2_units):\n",
    "def create_model(X_train,  y_train, window_size, specs):\n",
    "\n",
    "    # 1200, 132, 44\n",
    "    # 800, 800, 64\n",
    "    input_shape=(window_size, X_train.shape[1])\n",
    "    print(f\"Creating model({specs}), input shape={input_shape}\")\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(specs[0],  return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(specs[1]))\n",
    "    model.add(LSTM(specs[2]))         \n",
    "    model.add(Dropout(specs[3]))\n",
    "    model.add(Dense(specs[4], activation='relu'))\n",
    "    model.add(Dropout(specs[5]))\n",
    "    model.add(Dense(specs[6], activation='relu'))\n",
    "    model.add(Dropout(specs[7]))\n",
    "    model.add(Dense(y_train.shape[1], activation='linear')) # linear for regression\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_history(model_name, history, metric=None):\n",
    "    \"\"\" Plot history loss and metrics\"\"\"\n",
    "    metric_names = [metric] if metric else history.history\n",
    "    \n",
    "    # Print all merrics\n",
    "    for metric_name in metric_names:\n",
    "        # Validation metrics names will be calculated from related train metrics\n",
    "        if metric_name.startswith(\"val_\"): continue\n",
    "\n",
    "        # Plot metric and related test (val_..) metric\n",
    "        plt.figure(figsize=plot_figsize)\n",
    "        names=[metric_name, f\"val_{metric_name}\"]\n",
    "        for name in names:\n",
    "            plt.plot(history.history[name])\n",
    "        # Captions and show the plot\n",
    "        plt.title(f\"{model_name} {metric_name}\")\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(names, loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "def fit_model(model, train_gen, test_gen):\n",
    "    # Fit the model\n",
    "    epochs=50\n",
    "    steps_per_epoch=5\n",
    "    history=model.fit(train_gen, validation_data=test_gen, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_models(*specs):\n",
    "    results={}\n",
    "    for unit_spec in specs:\n",
    "        #print(unit_spec)\n",
    "        window_size=unit_spec[0]\n",
    "        model = create_model(X_train, y_train, window_size, unit_spec[1:])\n",
    "        \n",
    "        train_gen = TimeseriesGenerator(X_train, y_train, length=window_size)\n",
    "        test_gen = TimeseriesGenerator(X_test, y_test, length=window_size)        \n",
    "        history = fit_model(model, train_gen, test_gen)\n",
    "        \n",
    "        model_name = f\"Model({unit_spec})\"\n",
    "        results[model_name] = history\n",
    "    return results\n",
    "\n",
    "def plot_res(results):\n",
    "    for model_name in results:\n",
    "        plot_history(model_name, results[model_name], \"mse\")\n",
    "\n",
    "    \n",
    "# Good: 80, 512, 20\n",
    "base_model_res = evaluate_models([10,482,0.2,800, 0.13, 590, 0.29, 49, 0.22])\n",
    "plot_res(base_model_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Good models:\n",
    "#new_res = evaluate_models([10,320,0.2,160, 0.2, 40, 0.2, 16, 0.1])\n",
    "\n",
    "new_res = evaluate_models([10,320,0.2,160, 0.2, 40, 0.2, 16, 0.1], \n",
    "                          [10,825,0.1,588, 0.2, 190, 0.1, 59, 0.2])\n",
    "# Add previous base model to comparison\n",
    "res = {f\"Base {key}\":val for key,val in base_model_res.items()}\n",
    "res.update(new_res)\n",
    "\n",
    "plot_res(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
