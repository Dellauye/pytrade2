{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict low high models\n",
    "Model selection for predict lo hi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To install conda and binance packages to this notebook uncomment the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#%conda install -c plotly plotly=5.9.0\n",
    "#%conda install pip\n",
    "#%conda install twisted\n",
    "%pip install plotly==5.9.0\n",
    "%pip install twisted\n",
    "#%pip install binance-connector==1.13.0\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install tensorflow==2.11.0\n",
    "%pip install keras==2.11.0\n",
    "%pip install scikeras\n",
    "%pip install keras-tuner\n",
    "\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering\n",
    "Read data from bot's history, resample to equal intervals, create X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data from bot's history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./../data/yandex-cloud/LSTMStrategy/Xy/2023-05-27_BTCUSDT_data.csv', './../data/yandex-cloud/LSTMStrategy/Xy/2023-05-28_BTCUSDT_data.csv']\n",
      "['./../data/yandex-cloud/LSTMStrategy/Xy/2023-05-27_BTCUSDT_X.csv', './../data/yandex-cloud/LSTMStrategy/Xy/2023-05-28_BTCUSDT_X.csv']\n",
      "['./../data/yandex-cloud/LSTMStrategy/Xy/2023-05-27_BTCUSDT_y.csv', './../data/yandex-cloud/LSTMStrategy/Xy/2023-05-28_BTCUSDT_y.csv']\n",
      "Raw x len: 100000, raw y len: 100000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import timedelta,date\n",
    "\n",
    "def read_last_data(strategy, days=1, n=None):\n",
    "    \"\"\" Read last last day from bot history \"\"\"\n",
    "    \n",
    "    def read_last(symbol: str, name: str, days=1):\n",
    "        data_dir=f\"./../data/yandex-cloud/{strategy}/Xy\"\n",
    "        file_paths = sorted([f\"{data_dir}/{f}\" for f in os.listdir(data_dir) if f.endswith(f\"{symbol}_{name}.csv\")])[-days:]\n",
    "        print(file_paths)\n",
    "        return pd.concat([pd.read_csv(f, parse_dates=True, index_col=\"datetime\") for f in file_paths])\n",
    "\n",
    "    data=read_last(\"BTCUSDT\", \"data\", days)\n",
    "    X=read_last(\"BTCUSDT\", \"X\", days)\n",
    "    y=read_last(\"BTCUSDT\", \"y\", days)\n",
    "    if n: \n",
    "        data=data.tail(n)\n",
    "        X=X.tail(n)\n",
    "        y=y.tail(n)\n",
    "    return data,X,y\n",
    "\n",
    "#strategy=\"SimpleKerasStrategy\"\n",
    "strategy=\"LSTMStrategy\"\n",
    "bidask,X_bot,y_bot = read_last_data(strategy, days=2, n=100000)\n",
    "print(f\"Raw x len: {len(X_bot)}, raw y len: {len(y_bot)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resample data to use equal time intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X len: 100000, y len: 100000\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def get_targets(bidask, predict_window=\"10s\"):\n",
    "    \"\"\" \n",
    "    Calculate targets - bid/ask bounds in future prediction window\n",
    "    \"\"\"\n",
    "    fut_min=bidask[[\"bid\", \"ask\"]][::-1].rolling(predict_window).min()[::-1].rename(columns={\"bid\":\"bid_min_fut\", \"ask\":\"ask_min_fut\"})\n",
    "    fut_max=bidask[[\"bid\", \"ask\"]][::-1].rolling(predict_window).max()[::-1].rename(columns={\"bid\":\"bid_max_fut\", \"ask\":\"ask_max_fut\"})\n",
    "    return pd.concat([fut_min, fut_max], axis = 1)\n",
    "    \n",
    "def resampled(X: pd.DataFrame, y: pd.DataFrame, time_interval) -> (pd.DataFrame, pd.DataFrame):\n",
    "    \"\"\" Resample to make equal intervals time series \"\"\"\n",
    "    if int(re.sub(r'\\D', \"\", time_interval)) == 0:\n",
    "        return X, y\n",
    "    Xy=pd.concat([X,y], axis=1)\n",
    "    diffmap = [(c, \"sum\") for c in Xy.columns if c.endswith(\"diff\")]\n",
    "    timemap = [(c, \"last\") for c in Xy.columns if c.startswith(\"time\") and not c.endswith(\"diff\")]\n",
    "    l2map = [(c, \"last\") for c in Xy.columns if c.startswith(\"l2_\")]\n",
    "    candlemap = [(c,\"last\") for c in Xy.columns \\\n",
    "                 if c.endswith(\"_open\") or c.endswith(\"_high\") or c.endswith(\"_low\") or c.endswith(\"_close\") or c.endswith(\"_vol\")]\n",
    "    \n",
    "    futmap = [(c, \"last\") for c in Xy.columns if c.endswith(\"_fut\")]\n",
    "    colmap = dict(diffmap + timemap + l2map + futmap + [(\"spread\", \"last\")] + candlemap)\n",
    "    resampled = Xy.resample(time_interval).agg(colmap).dropna()\n",
    "    return resampled[X.columns], resampled[y.columns]\n",
    "\n",
    "predict_window=\"60s\"\n",
    "resample_interval=\"10s\"\n",
    "y_raw = get_targets(bidask, predict_window)\n",
    "X,y=X_bot,y_raw\n",
    "#X,y = resampled(X_bot, y_raw, resample_interval)\n",
    "print(f\"X len: {len(X)}, y len: {len(y)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 06:53:04.984950: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-28 06:53:05.098938: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-28 06:53:05.098963: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-28 06:53:05.589203: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-28 06:53:05.589269: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-28 06:53:05.589276: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 70000, test size: 30000\n",
      "X shape: (70000, 80), y shape: (70000, 4)\n",
      "X shape: (30000, 80), y shape: (30000, 4)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.layers import *\n",
    "from keras.layers import LSTM\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "\n",
    "def train_test_split(X,y):\n",
    "    test_size=0.3\n",
    "    test_index = int(len(X)*(1-test_size))\n",
    "    X_train, y_train, X_test, y_test = X.iloc[:test_index], y.iloc[:test_index], X.iloc[test_index:], y.iloc[test_index:]\n",
    "    time_cols=[col for col in X.columns if col.startswith(\"time\")]\n",
    "    float_cols = list(set(X.columns)-set(time_cols))\n",
    "\n",
    "    # Train/test split    \n",
    "#     x_scaler = \n",
    "\n",
    "    x_scaler = ColumnTransformer([(\"xrs\",RobustScaler(), float_cols)], remainder=\"passthrough\")\n",
    "\n",
    "    x_pipe = Pipeline(\n",
    "        [(\"xscaler\", ColumnTransformer([(\"xrs\",RobustScaler(), float_cols)], remainder=\"passthrough\")),\n",
    "         (\"xmms\",MinMaxScaler())])\n",
    "\n",
    "    \n",
    "    x_pipe.fit(X_train)\n",
    "\n",
    "    y_pipe = Pipeline(\n",
    "        [(\"yrs\", RobustScaler()),\n",
    "         (\"ymms\",MinMaxScaler())])\n",
    "    y_pipe.fit(y_train)\n",
    "\n",
    "    #x_scaler, y_scaler = MinMaxScaler().fit(X_train), MinMaxScaler().fit(y_train)\n",
    "    X_train, y_train = x_pipe.transform(X_train), y_pipe.transform(y_train)\n",
    "    X_test, y_test = x_pipe.transform(X_test), y_pipe.transform(y_test)\n",
    "    print(f\"Train size: {X_train.shape[0]}, test size: {X_test.shape[0]}\")\n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(X, y)\n",
    "print(f\"X shape: {X_train.shape}, y shape: {y_train.shape}\")\n",
    "print(f\"X shape: {X_test.shape}, y shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.layers import *\n",
    "from keras.layers import LSTM\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-28 06:53:06.866436: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-28 06:53:06.866521: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-28 06:53:06.866566: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-28 06:53:06.868124: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-28 06:53:06.868175: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2023-05-28 06:53:06.868216: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2023-05-28 06:53:06.868229: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2023-05-28 06:53:06.868928: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 8\n",
      "lstm1_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 1000, 'step': 1, 'sampling': 'linear'}\n",
      "lstm1_dropout (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': None, 'sampling': 'linear'}\n",
      "lstm2_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 1000, 'step': 1, 'sampling': 'linear'}\n",
      "lstm2_dropout (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': None, 'sampling': 'linear'}\n",
      "dense1_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 1000, 'step': 1, 'sampling': 'linear'}\n",
      "dense1_dropout (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': None, 'sampling': 'linear'}\n",
      "dense2_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 4, 'max_value': 100, 'step': 1, 'sampling': 'linear'}\n",
      "dense2_dropout (Float)\n",
      "{'default': 0.0, 'conditions': [], 'min_value': 0.0, 'max_value': 0.3, 'step': None, 'sampling': 'linear'}\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "855               |855               |lstm1_units\n",
      "0.072891          |0.072891          |lstm1_dropout\n",
      "727               |727               |lstm2_units\n",
      "0.26925           |0.26925           |lstm2_dropout\n",
      "45                |45                |dense1_units\n",
      "0.037293          |0.037293          |dense1_dropout\n",
      "39                |39                |dense2_units\n",
      "0.079713          |0.079713          |dense2_dropout\n",
      "\n",
      "Epoch 1/5\n",
      "547/547 [==============================] - 125s 223ms/step - loss: 0.1536 - mse: 0.0483 - val_loss: 1.2577 - val_mse: 2.2702\n",
      "Epoch 2/5\n",
      "523/547 [===========================>..] - ETA: 4s - loss: 0.1333 - mse: 0.0366"
     ]
    }
   ],
   "source": [
    "from keras import Sequential, Input\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from scikeras.wrappers import KerasRegressor\n",
    "from keras.layers import *\n",
    "from keras.layers import LSTM\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_tuner_model(hp):\n",
    "    \"\"\" Create model for tuner. Expects X_train, y_train, window_size\"\"\"\n",
    "    model = Sequential()\n",
    "    hp_lstm1_units=hp.Int(\"lstm1_units\", min_value=4, max_value=1000)\n",
    "    hp_lstm1_dropout=hp.Float(\"lstm1_dropout\", min_value=0, max_value=0.3)\n",
    "\n",
    "#     column_num=X_train.shape[1]\n",
    "#     window_sizes=[1,5,10,50] \n",
    "#     hp_window_size=hp.Choice(\"window_size\", window_sizes)\n",
    "    model.add(LSTM(hp_lstm1_units,\n",
    "                   return_sequences=True, input_shape=(window_size, X_train.shape[1])))\n",
    "#     model.add(LSTM(hp_input_units,\n",
    "#                    return_sequences=True, input_shape=(hp_window_size, column_num)))\n",
    "\n",
    "    model.add(Dropout(hp_lstm1_dropout))\n",
    "    \n",
    "    hp_lstm2_units=hp.Int(\"lstm2_units\", min_value=4, max_value=1000)\n",
    "    hp_lstm2_dropout=hp.Float(\"lstm2_dropout\", min_value=0, max_value=0.3)\n",
    "    model.add(LSTM(hp_lstm2_units))         \n",
    "    model.add(Dropout(hp_lstm2_dropout))\n",
    "\n",
    "    hp_dense1_units=hp.Int(\"dense1_units\", min_value=4, max_value=1000)\n",
    "    hp_dense1_dropout=hp.Float(\"dense1_dropout\", min_value=0, max_value=0.3)\n",
    "    model.add(Dense(hp_dense1_units, activation='relu'))\n",
    "    model.add(Dropout(hp_dense1_dropout))\n",
    "\n",
    "    hp_dense2_units=hp.Int(\"dense2_units\", min_value=4, max_value=100)\n",
    "    hp_dense2_dropout=hp.Float(\"dense2_dropout\", min_value=0, max_value=0.3)\n",
    "    model.add(Dense(hp_dense2_units, activation='relu'))\n",
    "    model.add(Dropout(hp_dense2_dropout))\n",
    "    \n",
    "    model.add(Dense(y_train.shape[1], activation='linear'))\n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "    return model\n",
    "    \n",
    "window_size = 10\n",
    "\n",
    "def create_tuner(max_trials):\n",
    "    # Tune the model\n",
    "    # tuner=keras_tuner.Hyperband(\n",
    "    #     hypermodel=create_tuner_model,\n",
    "    #     objective=\"val_mse\",\n",
    "    #     max_epochs=5,\n",
    "    #     factor=3,\n",
    "    #     hyperband_iterations=1,\n",
    "    #     seed=None,\n",
    "    #     hyperparameters=None,\n",
    "    #     tune_new_entries=True,\n",
    "    #     allow_new_entries=True,\n",
    "    #     max_retries_per_trial=0,\n",
    "    #     max_consecutive_failed_trials=3\n",
    "    # )\n",
    "    tuner=keras_tuner.BayesianOptimization(\n",
    "        hypermodel=create_tuner_model,\n",
    "        objective=\"val_mse\",\n",
    "        max_trials=max_trials,\n",
    "        num_initial_points=None,\n",
    "        alpha=0.0001,\n",
    "        beta=2.6,\n",
    "        seed=None,\n",
    "        hyperparameters=None,\n",
    "        tune_new_entries=True,\n",
    "        allow_new_entries=True,\n",
    "        max_retries_per_trial=0,\n",
    "        max_consecutive_failed_trials=3,\n",
    "        overwrite=True,\n",
    "        directory=f\"./tmp/{strategy}/tune\",\n",
    "        project_name=\"pytrade2\",    \n",
    "    )\n",
    "    # tuner = keras_tuner.RandomSearch(\n",
    "    #     hypermodel=create_tuner_model,\n",
    "    #     objective=\"val_mse\",\n",
    "    #     max_trials=3,\n",
    "    #     executions_per_trial=2,\n",
    "    #     overwrite=True,\n",
    "    #     directory=f\"./tmp/{strategy}/tune\",\n",
    "    #     project_name=\"pytrade2\",\n",
    "    #     )\n",
    "    tuner.search_space_summary()\n",
    "    return tuner\n",
    "\n",
    "def tuner_search(max_trials,train_gen, test_gen):\n",
    "    tuner = create_tuner(max_trials)\n",
    "    tuner.search(train_gen,epochs=5, validation_data=test_gen)\n",
    "    return tuner\n",
    "\n",
    "train_gen = TimeseriesGenerator(X_train, y_train, length=window_size)\n",
    "test_gen = TimeseriesGenerator(X_test, y_test, length=window_size)\n",
    "# Uncomment to do optimize\n",
    "tuner = tuner_search(1, train_gen, test_gen)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "os.environ.setdefault(\"TF_CPP_MIN_LOG_LEVEL\", \"1\")\n",
    "\n",
    "tuner.results_summary()\n",
    "models=tuner.get_best_models(5)\n",
    "for model in models:\n",
    "    model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# def create_model_prod(X_train, y_train, window_size):\n",
    "#     model = Sequential()\n",
    "#     model.add(LSTM(128,  return_sequences=True, input_shape=(window_size, X_train.shape[1])))\n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(LSTM(32))         \n",
    "#     model.add(Dropout(0.2))\n",
    "#     model.add(Dense(20, activation='relu'))\n",
    "#     model.add(Dense(y_train.shape[1], activation='linear'))\n",
    "#     #model.add(Dense(y_train.shape[1], activation='softmax'))\n",
    "#     model.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "#     return model  \n",
    "plot_figsize=(10,5)\n",
    "#def create_model(X_train, y_train, window_size, lstm1_units, lstm2_units, dense1_units, dense2_units):\n",
    "def create_model(X_train,  y_train, window_size, specs):\n",
    "\n",
    "    # 1200, 132, 44\n",
    "    # 800, 800, 64\n",
    "    input_shape=(window_size, X_train.shape[1])\n",
    "    print(f\"Creating model({specs}), input shape={input_shape}\")\n",
    "\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(LSTM(specs[0],  return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(specs[1]))\n",
    "    model.add(LSTM(specs[2]))         \n",
    "    model.add(Dropout(specs[3]))\n",
    "    model.add(Dense(specs[4], activation='relu'))\n",
    "    model.add(Dropout(specs[5]))\n",
    "    model.add(Dense(specs[6], activation='relu'))\n",
    "    model.add(Dropout(specs[7]))\n",
    "    model.add(Dense(y_train.shape[1], activation='linear')) # linear for regression\n",
    "\n",
    "    model.compile(optimizer='adam', loss='mae', metrics=['mse'])\n",
    "    model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def plot_history(model_name, history, metric=None):\n",
    "    \"\"\" Plot history loss and metrics\"\"\"\n",
    "    metric_names = [metric] if metric else history.history\n",
    "    \n",
    "    # Print all merrics\n",
    "    for metric_name in metric_names:\n",
    "        # Validation metrics names will be calculated from related train metrics\n",
    "        if metric_name.startswith(\"val_\"): continue\n",
    "\n",
    "        # Plot metric and related test (val_..) metric\n",
    "        plt.figure(figsize=plot_figsize)\n",
    "        names=[metric_name, f\"val_{metric_name}\"]\n",
    "        for name in names:\n",
    "            plt.plot(history.history[name])\n",
    "        # Captions and show the plot\n",
    "        plt.title(f\"{model_name} {metric_name}\")\n",
    "        plt.ylabel(metric_name)\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(names, loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "def fit_model(model, train_gen, test_gen):\n",
    "    # Fit the model\n",
    "    epochs=50\n",
    "    steps_per_epoch=5\n",
    "    history=model.fit(train_gen, validation_data=test_gen, epochs=epochs, steps_per_epoch=steps_per_epoch)\n",
    "    return history\n",
    "\n",
    "\n",
    "def evaluate_models(*specs):\n",
    "    results={}\n",
    "    for unit_spec in specs:\n",
    "        #print(unit_spec)\n",
    "        window_size=unit_spec[0]\n",
    "        model = create_model(X_train, y_train, window_size, unit_spec[1:])\n",
    "        \n",
    "        train_gen = TimeseriesGenerator(X_train, y_train, length=window_size)\n",
    "        test_gen = TimeseriesGenerator(X_test, y_test, length=window_size)        \n",
    "        history = fit_model(model, train_gen, test_gen)\n",
    "        \n",
    "        model_name = f\"Model({unit_spec})\"\n",
    "        results[model_name] = history\n",
    "    return results\n",
    "\n",
    "def plot_res(results):\n",
    "    for model_name in results:\n",
    "        plot_history(model_name, results[model_name], \"mse\")\n",
    "\n",
    "    \n",
    "# Good: 80, 512, 20\n",
    "base_model_res = evaluate_models([10,482,0.2,800, 0.13, 590, 0.29, 49, 0.22])\n",
    "plot_res(base_model_res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Good models:\n",
    "#new_res = evaluate_models([10,320,0.2,160, 0.2, 40, 0.2, 16, 0.1])\n",
    "\n",
    "new_res = evaluate_models([10,320,0.2,160, 0.2, 40, 0.2, 16, 0.1], \n",
    "                          [10,825,0.1,588, 0.2, 190, 0.1, 59, 0.2])\n",
    "# Add previous base model to comparison\n",
    "res = {f\"Base {key}\":val for key,val in base_model_res.items()}\n",
    "res.update(new_res)\n",
    "\n",
    "plot_res(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
